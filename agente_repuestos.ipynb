{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c400ec",
   "metadata": {},
   "source": [
    "# Trabajo Pr치ctico: Agente para automatizar la b칰squeda de repuestos\n",
    "\n",
    "- **Curso:** DUIA - 2025, M칩dulo 6\n",
    "- **Integrantes:** David Burckhardt, Martin Vazquez Arispe, Martin Caballero.\n",
    "- **Objetivo:** Implementar un sistema inteligente que automatice la b칰squeda, ranking y pedido de repuestos para una empresa distribuidora.\n",
    "\n",
    "---\n",
    "##  칈ndice del Notebook\n",
    "\n",
    "1. **Consigna del trabajo**\n",
    "2. **Configuracion de API Keys**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067266d9",
   "metadata": {},
   "source": [
    "## 1. Consigna: agente(s) para automatizar la b칰squeda de repuestos\n",
    "- Dada una solicitud de repuestos espec칤ficos para una empresa distribuidora, un\n",
    "agente debe identificar las especificaciones de dichos repuestos (seg칰n un\n",
    "cat치logo), a fin de poder buscarlos.\n",
    "- El agente busca en primer lugar en el inventario de la empresa, y en en caso de\n",
    "no encontrarlos (puede ser que encuentre solo algunos de ellos), debe consultar\n",
    "en cat치logos de proveedores.\n",
    "- El sistema extrae informaci칩n de las opciones encontradas, y genera un ranking\n",
    "de alternativas, priorizando: \n",
    "    - Repuestos internos (si est치n disponibles).\n",
    "    - Proveedores externos seg칰n criterios de optimizaci칩n (por ej. costo-beneficio).\n",
    "- Para repuestos internos: \n",
    "    - Se genera una orden de retiro del inventario y se notifica al almac칠n para su preparaci칩n. \n",
    "- Para repuestos externos: \n",
    "    - se env칤a un email automatizado al proveedor seleccionado para formalizar el pedido.\n",
    "\n",
    "- Finalmente, se agenda la fecha estimada de entrega y detalles del pedido en el\n",
    "sistema de seguimiento.\n",
    "- Pueden incluirse pasos de \"human in the loop\" para verificar resultados antes de\n",
    "tomar acciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574a6438",
   "metadata": {},
   "source": [
    "## 2. Configuraci칩n de API Keys y variables de entorno\n",
    "\n",
    "Cargamos la `GROQ_API_KEY` desde el archivo `.env` e inicializamos el cliente LLM.\n",
    "\n",
    "**Nota:** Aseg칰rate de tener un archivo `.env` en el directorio ra칤z con:\n",
    "```\n",
    "GROQ_API_KEY=tu_clave_aqui\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b03f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# Verificar que la API key est치 configurada\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"GROQ_API_KEY no encontrada en .env\")\n",
    "\n",
    "# Inicializar el LLM de Groq\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.1,\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "print(\"LLM de Groq inicializado correctamente\")\n",
    "print(f\"   Modelo: {llm.model_name}\")\n",
    "print(f\"   Temperature: {llm.temperature}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e004bc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "import json\n",
    "\n",
    "# Create a simple prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Mantene una conversacion con el usuario\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create the chain that guarantees JSON output\n",
    "chain = prompt | llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec9ae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "#Definimos el esquema\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "#Defino funcion del nodo\n",
    "def nodo_llm(state: AgentState) -> AgentState:\n",
    "    messages = state[\"messages\"]\n",
    "    response = chain.invoke(messages)\n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "    }\n",
    "\n",
    "def tiene_info_suficiente(state: AgentState) -> str:\n",
    "    # Logica para decidir si se continua o no\n",
    "    # Por ahora, siempre va a END\n",
    "    return \"continue\"\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "#Defino el grafo\n",
    "graph_builder = StateGraph(AgentState)\n",
    "\n",
    "#Agrego el nodo al grafo\n",
    "graph_builder.add_node(\"llm_node\", nodo_llm)\n",
    "\n",
    "#Conecto los nodos (Solo uno por ahora)\n",
    "graph_builder.add_edge(START, \"llm_node\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"llm_node\",\n",
    "    tiene_info_suficiente,\n",
    "    {\n",
    "        \"continue\": \"llm_node\",  # Vuelve a ejecutar llm_node\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "graph = graph_builder.compile(\n",
    "    checkpointer=memory, \n",
    "    interrupt_after=[\"llm_node\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e44229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defino la funcion del agente\n",
    "def iniciar_agente(mensaje_usuario: str):\n",
    "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "    estado_inicial = AgentState(\n",
    "        messages=[HumanMessage(content=mensaje_usuario)],\n",
    "    )\n",
    "    result = graph.invoke(estado_inicial, config)\n",
    "    end = False\n",
    "    while not end:\n",
    "        print(result[\"messages\"][-1].content)\n",
    "        nuevo_mensaje = input(\"游녻 T칰: \")\n",
    "        if nuevo_mensaje == \"salir\":\n",
    "            end = True\n",
    "        else:\n",
    "            nuevo_estado = {\"messages\": [HumanMessage(content=nuevo_mensaje)]}\n",
    "            result = graph.invoke(nuevo_estado, config)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ff072",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main\n",
    "mensaje_usuario = input(\"游녻 T칰: \")\n",
    "resultado = iniciar_agente(mensaje_usuario)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DUIA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
